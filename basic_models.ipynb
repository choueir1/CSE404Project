{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91f5545-3b1c-493c-9e52-ea4d00989cde",
   "metadata": {},
   "source": [
    "CSE 404 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736ba4c-0435-48ca-aa5c-3d98a17f2ea2",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61164e2-add4-4cd7-9aec-f6f13a6ef9d5",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "##### Exploratory Data Analysis (EDA)\n",
    "- Check the structure of the dataset:\n",
    "    - Verify column names and data types\n",
    "- Identify missing values:\n",
    "    - Drop weekends as they are non trading days.\n",
    "    - If missing values exist in **Open/Close** and **High/Low**: Fill with the average of previous and next day's values. \n",
    "    - If missing values exist in **Volume**: Fill with the median.\n",
    "- Identify duplicate rows:\n",
    "  - If duplicate trading days exist for the same stock, drop them.\n",
    "\n",
    "##### Split dataset into training, test, and validation set\n",
    "- Use **70/15/15 split** for training, testing, and validation\n",
    "\n",
    "\n",
    "##### Feature Scaling \n",
    "- Scale features using **StandardScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b948075-e937-4853-bcfa-8f143b979d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6660e81-2d31-47d9-8a59-e202b2eeeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "stocks = pd.read_csv(\"stocks.csv\")\n",
    "\n",
    "# Display basic info and first few rows\n",
    "print(stocks.columns)\n",
    "stocks.info() \n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3094d-543d-4eae-92fb-55a6518f7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Types:\",stocks.dtypes )\n",
    "print(\"Missing Values:\", stocks.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecb739-af46-49d8-93cd-85546a4282d9",
   "metadata": {},
   "source": [
    "Each stock has seperate columns for Close, High, Low, Open, and Volume. There are many NaN values in the dataset for stocks that did not exit or trade on a specific date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83415ed-4b8c-4642-9f53-6c45055d3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deal with missing values\n",
    "\n",
    "#Stock market closed on weekends so drop weekends\n",
    "stocks['Date_'] = pd.to_datetime(stocks['Date_'])\n",
    "stocks = stocks.loc[stocks['Date_'].dt.dayofweek < 5]\n",
    "#https://gpttutorpro.com/pandas-dataframe-filtering-using-datetime-methods/\n",
    "\n",
    "#Replace missing values in Open,Close, High, and Low columns\n",
    "for col in stocks.columns:\n",
    "    if \"Open\" in col or \"Close\" in col or \"High\" in col or \"Low\" in col:\n",
    "        # Find the first valid index where trading starts\n",
    "        first_valid_index = stocks[col].first_valid_index()\n",
    "\n",
    "        if first_valid_index is not None:\n",
    "            # Fill missing values only after the stock starts trading with average of previous and next day's values.\n",
    "            stocks.loc[first_valid_index:,col] = stocks.loc[first_valid_index:,col].fillna((stocks[col].shift(1)+stocks[col].shift(-1))/2)\n",
    "\n",
    "            # Forward-fill and backward-fill only after the first valid trading day (fills with closest)\n",
    "            stocks.loc[first_valid_index:,col] = stocks.loc[first_valid_index:,col].ffill().bfill()\n",
    "\n",
    "#https://medium.com/@farisyid/penggunaan-ffill-dan-bfill-pada-proses-data-cleaning-b4f3bfec9767#:~:text='ffill'%20which%20means%20forward%20fill%20and%20'bfill',such%20as%20DataFrame%20or%20Series%20in%20Pandas.&text=Instead%2C%20the%20'bfill'%20method%20fills%20the%20missing,the%20missing%20value%20in%20the%20data%20sequence.\n",
    "\n",
    "#replace missing values in volume columns with the median\n",
    "volume_cols = []\n",
    "for col in stocks.columns:\n",
    "    if \"Volume\" in col:\n",
    "        volume_cols.append(col)\n",
    "for col in volume_cols:\n",
    "    stocks.loc[:, col] = stocks[col].fillna(stocks[col].median())\n",
    "\n",
    "stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56035b11-0ba0-4084-9411-e1ec16ee0c72",
   "metadata": {},
   "source": [
    "Missing values in the dataset were handled by first removing weekends, as they are non-trading days. For Open, Close, High, and Low prices, missing values were filled using the average of the previous and next trading day's values, ensuring that data was only adjusted after the stock had begun trading. Volume data was completed using the median to maintain consistency and avoid skewing results. This approach ensures realistic stock data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebe903-4657-413f-90d3-1e8a36d19206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if duplicate rows exist\n",
    "print(\"Number of duplicate rows:\", stocks.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038224ea-b82f-4378-8b6e-276647f83b24",
   "metadata": {},
   "source": [
    "No duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b853c07-4bbd-4b21-8690-2466e05fbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.drop(['Unnamed: 0', 'Date_'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41917f9-1476-400c-bef4-06a1d7fae6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks[\"Percent_Change\"] = (stocks[\"Close_AAPL\"].shift(-1) - stocks[\"Close_AAPL\"]) / stocks[\"Close_AAPL\"]\n",
    "\n",
    "def action(per_change):\n",
    "    if per_change>0.01:\n",
    "        return 2 # buy\n",
    "    elif per_change<-0.01:\n",
    "        return 0 #sell\n",
    "    else:\n",
    "        return 1 #hold\n",
    "        \n",
    "stocks[\"Target\"] = stocks[\"Percent_Change\"].apply(action)\n",
    "stocks = stocks.drop(columns=[\"Percent_Change\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23287e-ba67-4b0d-825d-4ad20d7baa33",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f237cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Logistic Regression Setup\n",
    "# ---------------------------\n",
    "\n",
    "# Create the target variable:\n",
    "# Predict if Close_AAPL increases (1) or decreases (0) the next day.\n",
    "#stocks[\"Target\"] = (stocks[\"Close_AAPL\"].shift(-1) > stocks[\"Close_AAPL\"]).astype(int)\n",
    "\n",
    "# Drop the last row (no \"next day\" available)\n",
    "stocks = stocks[:-1]\n",
    "\n",
    "# Remove the Date column (non-numeric)\n",
    "if \"Date\" in stocks.columns:\n",
    "    stocks = stocks.drop(columns=[\"Date_\"])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = stocks.drop(columns=[\"Target\"])\n",
    "y = stocks[\"Target\"]\n",
    "\n",
    "# ----- Additional Safeguard: Impute any remaining missing values in features -----\n",
    "# This ensures that even if some NaNs were missed during cleaning, they are filled.\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Confirm no NaNs remain\n",
    "assert X.isnull().sum().sum() == 0, \"There are still missing values in the features!\"\n",
    "\n",
    "# ---------------------------\n",
    "# Split the Data\n",
    "# ---------------------------\n",
    "# Use a 70/15/15 split for training, validation, and testing.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Scaling\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# Train Logistic Regression Model\n",
    "# ---------------------------\n",
    "log_reg = LogisticRegression(max_iter=500, random_state=42, multi_class='multinomial')\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ---------------------------\n",
    "# Classification Report\n",
    "# ---------------------------\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Sell\", \"Hold\", \"Buy\"]))\n",
    "\n",
    "### only for binary\n",
    "# # ---------------------------\n",
    "# # 1. Plot the ROC Curve\n",
    "# # ---------------------------\n",
    "# # Get predicted probabilities for the positive class on the test set.\n",
    "# y_test_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# # Compute the ROC curve and the AUC.\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_test_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve for Logistic Regression Model')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Plot the Logistic Regression Coefficients\n",
    "# ---------------------------\n",
    "# Retrieve the coefficients and corresponding feature names.\n",
    "\n",
    "class_labels = {0: \"Sell\", 1: \"Hold\", 2: \"Buy\"}\n",
    "for i in range(3):\n",
    "    coef = log_reg.coef_[i]\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create a DataFrame and sort by absolute coefficient values.\n",
    "    coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coef})\n",
    "    coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "    coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "    \n",
    "    # Plot the top 20 features.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(coef_df['Feature'].head(20)[::-1], coef_df['Coefficient'].head(20)[::-1])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top 20 Features Influencing \"{class_labels[i]}\" Prediction')\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Plot the Confusion Matrix\n",
    "# ---------------------------\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Sell\", \"Hold\", \"Buy\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Multi-Class Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Evaluate the Model\n",
    "# ---------------------------\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_val_pred = log_reg.predict(X_val_scaled)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred, target_names=[\"Sell\", \"Hold\", \"Buy\"]))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_test_pred = log_reg.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}\")\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred, target_names=[\"Sell\", \"Hold\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc34552-b12c-493e-b78c-72b41264a48e",
   "metadata": {},
   "source": [
    "### FF Neural Network Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150b58e-5ff6-4a55-89a6-1b10120fa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f48eb4-80e7-4de5-a15e-7cdeffe15e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(X_train.shape[1])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
    "y_test_probs = model.predict(X_test_scaled)\n",
    "y_val_probs = model.predict(X_val_scaled)\n",
    "y_test_pred_nn = np.argmax(y_test_probs, axis=1)\n",
    "y_val_pred_nn  = np.argmax(y_val_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68515973-0309-4eb9-b644-a5d49499a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'\\nTest Accuracy: {accuracy:.2f}')\n",
    "print(\"\\nTest Set Classification Report:\\n\", classification_report(y_test, y_test_pred_nn, target_names=[\"Sell\", \"Hold\", \"Buy\"]))\n",
    "print(\"\\nValidation Set Classification Report:\\n\", classification_report(y_val, y_val_pred_nn, target_names=[\"Sell\", \"Hold\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145335b-2cea-45ff-bae5-35a5690f51f7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
